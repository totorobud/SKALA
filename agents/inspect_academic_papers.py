#!/usr/bin/env python3
"""
academic_papers ë°ì´í„° êµ¬ì¡° í™•ì¸ ìŠ¤í¬ë¦½íŠ¸

ì‹¤í–‰ ë°©ë²•:
    python inspect_academic_papers.py
"""

import os
import sys
import json
from collections import defaultdict, Counter

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from bio_agent_code.state import PipelineState, TrendItem
from bio_agent_code.agents.Trend_Profiling_Agent import run


def inspect_paper_item(paper, index=0):
    """PaperItem ê°ì²´ì˜ ìƒì„¸ ì •ë³´ ì¶œë ¥"""
    print(f"\n{'='*80}")
    print(f"ğŸ“„ Paper #{index + 1}")
    print(f"{'='*80}")
    print(f"Trend Title:    {paper.trend_title}")
    print(f"Paper Title:    {paper.paper_title}")
    print(f"Authors:        {paper.authors or '(ì—†ìŒ)'}")
    print(f"Year:           {paper.year}")
    print(f"URL:            {paper.url}")
    print(f"\nAbstract (ì²˜ìŒ 200ì):")
    print(f"  {paper.abstract[:200]}...")
    print(f"\nCitation:")
    print(f"  {paper.citation}")
    print(f"\nTech Summary (ì²˜ìŒ 300ì):")
    print(f"  {paper.tech_summary[:300]}...")
    print(f"\ní•„ë“œ ê¸¸ì´:")
    print(f"  - abstract: {len(paper.abstract)} ë¬¸ì")
    print(f"  - tech_summary: {len(paper.tech_summary)} ë¬¸ì")


def analyze_academic_papers(state):
    """academic_papers ë°ì´í„° ì „ì²´ ë¶„ì„"""
    
    papers = state.academic_papers
    
    if not papers:
        print("âš ï¸  academic_papersê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
        return
    
    print("\n" + "="*80)
    print("ğŸ“Š ACADEMIC PAPERS ë°ì´í„° ë¶„ì„")
    print("="*80)
    
    # 1. ê¸°ë³¸ í†µê³„
    print(f"\n1ï¸âƒ£ ê¸°ë³¸ í†µê³„:")
    print(f"   ì´ ë…¼ë¬¸ ìˆ˜: {len(papers)}ê°œ")
    print(f"   íŠ¸ë Œë“œ ìˆ˜: {len(set(p.trend_title for p in papers))}ê°œ")
    
    # 2. íŠ¸ë Œë“œë³„ ë¶„í¬
    print(f"\n2ï¸âƒ£ íŠ¸ë Œë“œë³„ ë…¼ë¬¸ ìˆ˜:")
    trend_counts = Counter(p.trend_title for p in papers)
    for trend, count in trend_counts.most_common():
        print(f"   â€¢ {trend}: {count}ê°œ")
    
    # 3. ì—°ë„ë³„ ë¶„í¬
    print(f"\n3ï¸âƒ£ ì—°ë„ë³„ ë…¼ë¬¸ ìˆ˜:")
    year_counts = Counter(p.year for p in papers)
    for year, count in sorted(year_counts.items(), reverse=True):
        print(f"   â€¢ {year}: {count}ê°œ")
    
    # 4. íŠ¸ë Œë“œë³„ ê·¸ë£¹í™”
    print(f"\n4ï¸âƒ£ íŠ¸ë Œë“œë³„ ìƒì„¸ ì •ë³´:")
    trends_dict = defaultdict(list)
    for paper in papers:
        trends_dict[paper.trend_title].append(paper)
    
    for trend_name, trend_papers in trends_dict.items():
        print(f"\n   [{trend_name}]")
        print(f"   ë…¼ë¬¸ ìˆ˜: {len(trend_papers)}ê°œ")
        
        # ì—°ë„ ë¶„í¬
        years = Counter(p.year for p in trend_papers)
        print(f"   ì—°ë„ ë¶„í¬: {dict(years)}")
        
        # ì €ì ì •ë³´ í†µê³„
        has_authors = sum(1 for p in trend_papers if p.authors)
        print(f"   ì €ì ì •ë³´ ìˆìŒ: {has_authors}ê°œ / {len(trend_papers)}ê°œ")
        
        # ìƒìœ„ 3ê°œ ë…¼ë¬¸
        print(f"   ìƒìœ„ 3ê°œ ë…¼ë¬¸:")
        for i, paper in enumerate(trend_papers[:3], 1):
            print(f"      {i}. {paper.paper_title[:50]}... ({paper.year})")
    
    # 5. í•„ë“œë³„ í†µê³„
    print(f"\n5ï¸âƒ£ í•„ë“œë³„ í†µê³„:")
    
    # abstract ê¸¸ì´
    abstract_lengths = [len(p.abstract) for p in papers]
    print(f"   Abstract ê¸¸ì´:")
    print(f"      í‰ê· : {sum(abstract_lengths) / len(abstract_lengths):.1f}ì")
    print(f"      ìµœì†Œ: {min(abstract_lengths)}ì")
    print(f"      ìµœëŒ€: {max(abstract_lengths)}ì")
    
    # tech_summary ê¸¸ì´
    tech_summary_lengths = [len(p.tech_summary) for p in papers]
    print(f"   Tech Summary ê¸¸ì´:")
    print(f"      í‰ê· : {sum(tech_summary_lengths) / len(tech_summary_lengths):.1f}ì")
    print(f"      ìµœì†Œ: {min(tech_summary_lengths)}ì")
    print(f"      ìµœëŒ€: {max(tech_summary_lengths)}ì")
    
    # ê³ ìœ í•œ tech_summary ê°œìˆ˜
    unique_summaries = len(set(p.tech_summary for p in papers))
    print(f"   ê³ ìœ í•œ Tech Summary: {unique_summaries}ê°œ")
    
    # 6. ë°ì´í„° í’ˆì§ˆ ì²´í¬
    print(f"\n6ï¸âƒ£ ë°ì´í„° í’ˆì§ˆ ì²´í¬:")
    
    empty_authors = sum(1 for p in papers if not p.authors)
    print(f"   â€¢ ì €ì ì •ë³´ ì—†ìŒ: {empty_authors}ê°œ ({empty_authors/len(papers)*100:.1f}%)")
    
    empty_abstracts = sum(1 for p in papers if not p.abstract)
    print(f"   â€¢ Abstract ì—†ìŒ: {empty_abstracts}ê°œ ({empty_abstracts/len(papers)*100:.1f}%)")
    
    empty_urls = sum(1 for p in papers if not p.url)
    print(f"   â€¢ URL ì—†ìŒ: {empty_urls}ê°œ ({empty_urls/len(papers)*100:.1f}%)")
    
    missing_summaries = sum(1 for p in papers if not p.tech_summary)
    print(f"   â€¢ Tech Summary ì—†ìŒ: {missing_summaries}ê°œ ({missing_summaries/len(papers)*100:.1f}%)")


def export_to_json(state, filename="academic_papers.json"):
    """academic_papersë¥¼ JSON íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
    papers_data = []
    
    for paper in state.academic_papers:
        papers_data.append({
            "trend_title": paper.trend_title,
            "paper_title": paper.paper_title,
            "authors": paper.authors,
            "year": paper.year,
            "abstract": paper.abstract,
            "citation": paper.citation,
            "url": paper.url,
            "tech_summary": paper.tech_summary
        })
    
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(papers_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {filename}")
    print(f"   íŒŒì¼ í¬ê¸°: {os.path.getsize(filename):,} bytes")


def main():
    print("\n" + "="*80)
    print("ğŸ” ACADEMIC PAPERS ë°ì´í„° êµ¬ì¡° ê²€ì‚¬")
    print("="*80)
    
    # ìƒ˜í”Œ ë°ì´í„°ë¡œ Agent ì‹¤í–‰
    print("\nğŸ“Š ìƒ˜í”Œ ë°ì´í„°ë¡œ Agent ì‹¤í–‰ ì¤‘...")
    
    sample_trends = [
        TrendItem(
            title="AI Drug Discovery",
            summary="AI ì‹ ì•½ ê°œë°œ",
            sources=["https://example.com"],
            tags=["ai"],
            score=0.95
        ),
    ]
    
    state = PipelineState(target_count=1)
    state.discovered = sample_trends
    
    print("âš ï¸  ì£¼ì˜: Tavily APIë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì‹¤ì œ ê²€ìƒ‰ì´ ìˆ˜í–‰ë©ë‹ˆë‹¤.\n")
    
    try:
        result_state = run(state)
        
        if not result_state.academic_papers:
            print("âš ï¸  ë…¼ë¬¸ì´ ìˆ˜ì§‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            return
        
        # ì „ì²´ ë¶„ì„
        analyze_academic_papers(result_state)
        
        # ì²« ë²ˆì§¸ ë…¼ë¬¸ ìƒì„¸ ì •ë³´
        print("\n" + "="*80)
        print("ğŸ“‹ ìƒ˜í”Œ ë…¼ë¬¸ ìƒì„¸ ì •ë³´")
        print("="*80)
        inspect_paper_item(result_state.academic_papers[0], 0)
        
        # JSON ë‚´ë³´ë‚´ê¸°
        export_to_json(result_state, "academic_papers_sample.json")
        
        # ì‚¬ìš© ì˜ˆì‹œ
        print("\n" + "="*80)
        print("ğŸ’¡ ë‹¤ìŒ Agentì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•")
        print("="*80)
        print("""
# íŠ¸ë Œë“œë³„ë¡œ ê·¸ë£¹í™”
from collections import defaultdict

trends = defaultdict(list)
for paper in state.academic_papers:
    trends[paper.trend_title].append(paper)

# ê° íŠ¸ë Œë“œì˜ ë…¼ë¬¸ ìˆœíšŒ
for trend_name, papers in trends.items():
    print(f"\\n=== {trend_name} ===")
    print(f"ë…¼ë¬¸ ìˆ˜: {len(papers)}")
    print(f"ê¸°ìˆ  ìš”ì•½: {papers[0].tech_summary[:100]}...")
    
    for paper in papers:
        print(f"  - {paper.paper_title} ({paper.year})")
        print(f"    {paper.url}")
        """)
        
        print("\nâœ… ê²€ì‚¬ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()