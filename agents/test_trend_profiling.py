from __future__ import annotations
import os
import re
import traceback
from datetime import datetime
from typing import Dict, List, Optional

from dotenv import load_dotenv
load_dotenv()

# --- ì™¸ë¶€ ì˜ì¡´ ---
try:
    from tavily import TavilyClient
    _HAS_TAVILY = True
except Exception as e:
    _HAS_TAVILY = False
    print(f"[WARNING] Tavily import failed: {e}")

try:
    from anthropic import Anthropic
    _HAS_ANTHROPIC = True
except Exception as e:
    _HAS_ANTHROPIC = False
    print(f"[WARNING] Anthropic import failed: {e}")

# --- ë‚´ë¶€ ì˜ì¡´ ---
from ..state import PipelineState, TrendItem, PaperItem

# ---------------------------------------------------------
# ì„¤ì •
# ---------------------------------------------------------
PAPERS_PER_TREND = 10  # ê° íŠ¸ë Œë“œë‹¹ ìµœê·¼ ë…¼ë¬¸ 10ê°œ
SCHOLAR_SITE = "site:scholar.google.com"
RECENT_YEARS = 3  # ìµœê·¼ 3ë…„ ë…¼ë¬¸ ìš°ì„ 

# ---------------------------------------------------------
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ---------------------------------------------------------
def _norm(x: str) -> str:
    return (x or "").strip()

def _extract_year(text: str) -> Optional[int]:
    """í…ìŠ¤íŠ¸ì—ì„œ ì—°ë„ ì¶”ì¶œ (2020-2025 ë²”ìœ„)"""
    matches = re.findall(r'\b(202[0-5])\b', text)
    if matches:
        return int(matches[0])
    return None

def _parse_scholar_results(results: List[Dict], state: PipelineState) -> List[Dict]:
    """
    ì›¹ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë…¼ë¬¸ ì •ë³´ íŒŒì‹±
    
    ë°˜í™˜ í˜•ì‹:
    {
        'title': str,
        'authors': str,
        'year': int,
        'abstract': str,
        'citation': str,
        'url': str
    }
    """
    state.log(f"[Profiling] Parsing {len(results)} search results...")
    
    papers = []
    current_year = datetime.now().year
    cutoff_year = current_year - RECENT_YEARS
    
    for idx, r in enumerate(results, 1):
        try:
            title = _norm(r.get("title", ""))
            content = _norm(r.get("content", ""))
            url = _norm(r.get("url", ""))
            
            if not title or not url:
                state.log(f"[Profiling] Skipping result {idx}: missing title or URL")
                continue
            
            # ì—°ë„ ì¶”ì¶œ (ì œëª© ë˜ëŠ” ë‚´ìš©ì—ì„œ)
            year = _extract_year(f"{title} {content}")
            
            # ìµœê·¼ ë…¼ë¬¸ë§Œ í•„í„°ë§
            if year and year < cutoff_year:
                state.log(f"[Profiling] Skipping result {idx}: year {year} < cutoff {cutoff_year}")
                continue
            
            # ì €ì ì •ë³´ ì¶”ì¶œ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
            authors = ""
            if " - " in content:
                parts = content.split(" - ")
                if len(parts) > 1:
                    authors = parts[0][:200]  # ì €ì ë¶€ë¶„ ì œí•œ
            
            # AbstractëŠ” contentì˜ ì¼ë¶€ (ì²˜ìŒ 500ì)
            abstract = content[:500] if content else ""
            
            # ì¸ìš© êµ¬ë¬¸ ìƒì„± (APA ìŠ¤íƒ€ì¼ ê·¼ì‚¬)
            citation = _generate_citation(title, authors, year)
            
            papers.append({
                'title': title,
                'authors': authors,
                'year': year or current_year,
                'abstract': abstract,
                'citation': citation,
                'url': url
            })
            
            state.log(f"[Profiling] âœ“ Parsed paper {idx}: {title[:50]}... ({year or current_year})")
            
        except Exception as e:
            state.log(f"[Profiling] âœ— Error parsing result {idx}: {str(e)}")
            continue
    
    # ì—°ë„ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
    papers.sort(key=lambda x: x['year'], reverse=True)
    
    state.log(f"[Profiling] Successfully parsed {len(papers)} papers from {len(results)} results")
    
    return papers[:PAPERS_PER_TREND]

def _generate_citation(title: str, authors: str, year: Optional[int]) -> str:
    """ê°„ë‹¨í•œ ì¸ìš© êµ¬ë¬¸ ìƒì„±"""
    year_str = str(year) if year else "n.d."
    authors_short = authors[:100] + "..." if len(authors) > 100 else authors
    
    if authors_short:
        return f"{authors_short} ({year_str}). {title}"
    else:
        return f"({year_str}). {title}"

def _web_search_scholar(query: str, state: PipelineState, max_results: int = 20) -> List[Dict]:
    """
    Google Scholarì—ì„œ ë…¼ë¬¸ ê²€ìƒ‰ (Tavily API ì‚¬ìš©)
    """
    state.log(f"[Profiling] Starting web search for: '{query}'")
    
    if not _HAS_TAVILY:
        state.log("[Profiling] âœ— ERROR: Tavily library not available")
        return []
    
    api_key = os.getenv("TAVILY_API_KEY")
    if not api_key:
        state.log("[Profiling] âœ— ERROR: TAVILY_API_KEY not found in environment")
        return []
    
    try:
        client = TavilyClient(api_key=api_key)
        
        # Google Scholar ì‚¬ì´íŠ¸ ì œí•œ ê²€ìƒ‰
        search_query = f"{query} {SCHOLAR_SITE}"
        state.log(f"[Profiling] Search query: '{search_query}'")
        
        result = client.search(
            query=search_query,
            max_results=max_results,
            search_depth="basic",
            include_answer=False,
            include_raw_content=False
        )
        
        results = result.get("results", [])
        state.log(f"[Profiling] âœ“ Tavily returned {len(results)} results")
        
        formatted = []
        for r in results:
            formatted.append({
                "title": _norm(r.get("title", "")),
                "content": _norm(r.get("content", "")),
                "url": _norm(r.get("url", "")),
            })
        
        return formatted
        
    except Exception as e:
        state.log(f"[Profiling] âœ— ERROR during web search: {str(e)}")
        state.log(f"[Profiling] Stack trace: {traceback.format_exc()}")
        return []

def _summarize_technology(trend_title: str, papers: List[Dict], state: PipelineState) -> str:
    """
    Claude APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë…¼ë¬¸ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ê¸°ìˆ  ìš”ì•½ ìƒì„±
    """
    state.log(f"[Profiling] Generating technology summary for: {trend_title}")
    
    if not _HAS_ANTHROPIC:
        state.log("[Profiling] WARNING: Anthropic library not available, using fallback")
        return _fallback_summary(trend_title, papers, state)
    
    api_key = os.getenv("ANTHROPIC_API_KEY")
    if not api_key:
        state.log("[Profiling] WARNING: ANTHROPIC_API_KEY not found, using fallback")
        return _fallback_summary(trend_title, papers, state)
    
    try:
        client = Anthropic(api_key=api_key)
        
        # ë…¼ë¬¸ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
        papers_context = "\n\n".join([
            f"ë…¼ë¬¸ {i+1}:\nì œëª©: {p['title']}\nì €ì: {p['authors']}\nì—°ë„: {p['year']}\nì´ˆë¡: {p['abstract']}"
            for i, p in enumerate(papers[:5])  # ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©
        ])
        
        state.log(f"[Profiling] Using top {min(5, len(papers))} papers for summary")
        
        prompt = f"""ë‹¤ìŒì€ "{trend_title}" íŠ¸ë Œë“œì™€ ê´€ë ¨ëœ ìµœê·¼ í•™ìˆ  ë…¼ë¬¸ë“¤ì…ë‹ˆë‹¤.

{papers_context}

ìœ„ ë…¼ë¬¸ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ "{trend_title}" ê¸°ìˆ ì— ëŒ€í•´ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

1. ê¸°ìˆ  ê°œìš” (2-3ë¬¸ì¥)
2. ì£¼ìš” íŠ¹ì§• ë° ì¥ì  (3-4ê°œ í¬ì¸íŠ¸)
3. í˜„ì¬ ì—°êµ¬ ë™í–¥ (2-3ë¬¸ì¥)
4. í–¥í›„ ì „ë§ (1-2ë¬¸ì¥)

ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

        state.log("[Profiling] Calling Claude API...")
        
        message = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=1500,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        
        summary = message.content[0].text
        state.log(f"[Profiling] âœ“ Claude API returned summary ({len(summary)} chars)")
        
        return summary
        
    except Exception as e:
        state.log(f"[Profiling] âœ— ERROR calling Claude API: {str(e)}")
        state.log(f"[Profiling] Stack trace: {traceback.format_exc()}")
        return _fallback_summary(trend_title, papers, state)

def _fallback_summary(trend_title: str, papers: List[Dict], state: PipelineState) -> str:
    """API ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ìš”ì•½ ìƒì„±"""
    state.log("[Profiling] Using fallback summary generation")
    
    paper_count = len(papers)
    recent_years = sorted(list(set([p['year'] for p in papers])), reverse=True)[:3]
    
    summary = f"""
{trend_title} ê¸°ìˆ  ìš”ì•½

ìµœê·¼ {RECENT_YEARS}ë…„ê°„ {paper_count}ê°œì˜ ì£¼ìš” ë…¼ë¬¸ì´ ë°œí‘œë˜ì—ˆìŠµë‹ˆë‹¤.
ì—°êµ¬ê°€ í™œë°œí•œ ì—°ë„: {', '.join(map(str, recent_years))}

ì£¼ìš” ì—°êµ¬ ë¶„ì•¼:
"""
    
    # ìƒìœ„ 3ê°œ ë…¼ë¬¸ ì œëª© ë‚˜ì—´
    for i, p in enumerate(papers[:3], 1):
        summary += f"\n{i}. {p['title']} ({p['year']})"
    
    summary += f"\n\nì´ ê¸°ìˆ ì€ ìƒëª…ê³¼í•™ ë¶„ì•¼ì—ì„œ AIë¥¼ í™œìš©í•˜ì—¬ í˜ì‹ ì„ ì´ë£¨ê³  ìˆìœ¼ë©°, ì§€ì†ì ì¸ ì—°êµ¬ ê°œë°œì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤."
    
    return summary

def _print_results_summary(state: PipelineState) -> None:
    """
    í„°ë¯¸ë„ì— ìµœì¢… ê²°ê³¼ ì¶œë ¥
    """
    print("\n" + "="*80)
    print("ğŸ“Š TREND PROFILING AGENT - RESULTS SUMMARY")
    print("="*80)
    
    if not state.academic_papers:
        print("âš ï¸  No academic papers collected")
        return
    
    # íŠ¸ë Œë“œë³„ë¡œ ê·¸ë£¹í™”
    trends_data = {}
    for paper in state.academic_papers:
        if paper.trend_title not in trends_data:
            trends_data[paper.trend_title] = []
        trends_data[paper.trend_title].append(paper)
    
    print(f"\nâœ“ Total Trends Analyzed: {len(trends_data)}")
    print(f"âœ“ Total Papers Collected: {len(state.academic_papers)}")
    print("\n" + "-"*80)
    
    # ê° íŠ¸ë Œë“œë³„ ìƒì„¸ ì •ë³´
    for idx, (trend_title, papers) in enumerate(trends_data.items(), 1):
        print(f"\n[{idx}] TREND: {trend_title}")
        print(f"    Papers Found: {len(papers)}")
        
        # ì—°ë„ ë¶„í¬
        years = [p.year for p in papers]
        year_counts = {}
        for y in years:
            year_counts[y] = year_counts.get(y, 0) + 1
        print(f"    Year Distribution: {dict(sorted(year_counts.items(), reverse=True))}")
        
        # ê¸°ìˆ  ìš”ì•½ ë¯¸ë¦¬ë³´ê¸°
        if papers and papers[0].tech_summary:
            summary_preview = papers[0].tech_summary[:150].replace('\n', ' ')
            print(f"    Tech Summary: {summary_preview}...")
        
        # ìƒìœ„ 3ê°œ ë…¼ë¬¸
        print(f"    Top Papers:")
        for i, paper in enumerate(papers[:3], 1):
            print(f"      {i}. {paper.paper_title[:60]}... ({paper.year})")
            print(f"         Authors: {paper.authors[:50] if paper.authors else 'N/A'}...")
            print(f"         URL: {paper.url}")
    
    print("\n" + "="*80)
    print("âœ“ Profiling Complete - Data saved to state.academic_papers")
    print("="*80 + "\n")

# ---------------------------------------------------------
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ---------------------------------------------------------
def run(state: PipelineState) -> PipelineState:
    """
    Trend Profiling Agent
    
    í”„ë¡œì„¸ìŠ¤:
    1. state.discoveredì—ì„œ íŠ¸ë Œë“œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    2. ê° íŠ¸ë Œë“œì— ëŒ€í•´ Google Scholar ê²€ìƒ‰
    3. ìµœê·¼ ë…¼ë¬¸ 10ê°œ ì¶”ì¶œ (ì œëª©, abstract, ì¸ìš©êµ¬ë¬¸)
    4. ë…¼ë¬¸ ê¸°ë°˜ ê¸°ìˆ  ìš”ì•½ ìƒì„±
    5. state.academic_papersì— ê²°ê³¼ ì €ì¥
    6. í„°ë¯¸ë„ì— ê²°ê³¼ ì¶œë ¥
    """
    print("\n" + "="*80)
    print("ğŸ”¬ TREND PROFILING AGENT - START")
    print("="*80)
    
    state.log("[Profiling] ========== START ==========")
    state.log(f"[Profiling] Timestamp: {datetime.now().isoformat()}")
    
    if not state.discovered:
        state.log("[Profiling] âœ— ERROR: No trends to analyze (state.discovered is empty)")
        print("âš ï¸  No trends found in state.discovered - Skipping profiling")
        return state
    
    state.log(f"[Profiling] Found {len(state.discovered)} trends to analyze")
    print(f"âœ“ Processing {len(state.discovered)} trends from Discovery Agent\n")
    
    # API í‚¤ ì²´í¬
    tavily_key = os.getenv("TAVILY_API_KEY")
    anthropic_key = os.getenv("ANTHROPIC_API_KEY")
    
    state.log(f"[Profiling] TAVILY_API_KEY: {'âœ“ Found' if tavily_key else 'âœ— Missing'}")
    state.log(f"[Profiling] ANTHROPIC_API_KEY: {'âœ“ Found' if anthropic_key else 'âœ— Missing'}")
    
    if not tavily_key:
        print("âš ï¸  WARNING: TAVILY_API_KEY not found - search will fail")
    if not anthropic_key:
        print("âš ï¸  WARNING: ANTHROPIC_API_KEY not found - using fallback summaries")
    
    all_papers: List[PaperItem] = []
    
    # ê° íŠ¸ë Œë“œ ì²˜ë¦¬
    for trend_idx, trend in enumerate(state.discovered, 1):
        trend_title = trend.title
        
        print(f"\n[{trend_idx}/{len(state.discovered)}] Processing: {trend_title}")
        state.log(f"[Profiling] ========== TREND {trend_idx}/{len(state.discovered)}: {trend_title} ==========")
        
        try:
            # Google Scholar ê²€ìƒ‰ ìˆ˜í–‰
            state.log(f"[Profiling] Step 1: Web search")
            results = _web_search_scholar(trend_title, state, max_results=20)
            
            if not results:
                state.log(f"[Profiling] âœ— No search results for: {trend_title}")
                print(f"  âš ï¸  No search results found - skipping")
                continue
            
            # ë…¼ë¬¸ íŒŒì‹±
            state.log(f"[Profiling] Step 2: Parse papers")
            papers = _parse_scholar_results(results, state)
            
            if not papers:
                state.log(f"[Profiling] âœ— No valid papers after parsing for: {trend_title}")
                print(f"  âš ï¸  No valid papers found after parsing - skipping")
                continue
            
            print(f"  âœ“ Found {len(papers)} papers")
            
            # ê¸°ìˆ  ìš”ì•½ ìƒì„±
            state.log(f"[Profiling] Step 3: Generate tech summary")
            tech_summary = _summarize_technology(trend_title, papers, state)
            
            # PaperItem ìƒì„±
            state.log(f"[Profiling] Step 4: Create PaperItem objects")
            for paper_idx, paper in enumerate(papers, 1):
                paper_item = PaperItem(
                    trend_title=trend_title,
                    paper_title=paper['title'],
                    authors=paper['authors'],
                    year=paper['year'],
                    abstract=paper['abstract'],
                    citation=paper['citation'],
                    url=paper['url'],
                    tech_summary=tech_summary
                )
                all_papers.append(paper_item)
                state.log(f"[Profiling] Created PaperItem {paper_idx}/{len(papers)}")
            
            print(f"  âœ“ Generated tech summary ({len(tech_summary)} chars)")
            state.log(f"[Profiling] âœ“ Successfully processed trend: {trend_title}")
            
        except Exception as e:
            state.log(f"[Profiling] âœ— CRITICAL ERROR processing {trend_title}: {str(e)}")
            state.log(f"[Profiling] Stack trace:\n{traceback.format_exc()}")
            print(f"  âœ— ERROR: {str(e)}")
            continue
    
    # ìƒíƒœì— ì €ì¥
    state.academic_papers = all_papers
    state.log(f"[Profiling] ========== COMPLETE ==========")
    state.log(f"[Profiling] Total papers collected: {len(all_papers)}")
    state.log(f"[Profiling] Trends processed: {len(set(p.trend_title for p in all_papers))}")
    
    # í„°ë¯¸ë„ì— ê²°ê³¼ ì¶œë ¥
    _print_results_summary(state)
    
    return state


# ---------------------------------------------------------
# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ìš© ë©”ì¸ ë¸”ë¡
# ---------------------------------------------------------
if __name__ == "__main__":
    import sys
    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
    
    print("\n" + "="*80)
    print("ğŸ§ª TREND PROFILING AGENT - TEST MODE")
    print("="*80)
    
    from bio_agent_code.state import PipelineState, TrendItem
    
    # ìƒ˜í”Œ íŠ¸ë Œë“œ ë°ì´í„° ìƒì„± (Discovery Agent ê²°ê³¼ ì‹œë®¬ë ˆì´ì…˜)
    sample_trends = [
        TrendItem(
            title="AI Drug Discovery",
            summary="AIë¥¼ í™œìš©í•œ ì‹ ì•½ ê°œë°œ íŠ¸ë Œë“œ",
            sources=["https://labiotech.eu", "https://fiercebiotech.com"],
            tags=["discovery", "news"],
            score=0.95
        ),
        TrendItem(
            title="Protein Folding AI",
            summary="AlphaFold ë“± ë‹¨ë°±ì§ˆ êµ¬ì¡° ì˜ˆì¸¡ AI",
            sources=["https://labiotech.eu"],
            tags=["discovery", "news"],
            score=0.90
        ),
    ]
    
    state = PipelineState(target_count=2)
    state.discovered = sample_trends
    
    print(f"\nâœ“ Created test state with {len(sample_trends)} sample trends:")
    for i, trend in enumerate(sample_trends, 1):
        print(f"   {i}. {trend.title} (score: {trend.score})")
    
    print("\nğŸš€ Starting Trend Profiling Agent...\n")
    
    # Agent ì‹¤í–‰
    try:
        result_state = run(state)
        
        print(f"\n" + "="*80)
        print("âœ… TEST COMPLETE!")
        print("="*80)
        print(f"Papers collected: {len(result_state.academic_papers)}")
        print(f"Logs generated: {len(result_state._logs)}")
        
        if result_state.academic_papers:
            print(f"\nFirst paper example:")
            paper = result_state.academic_papers[0]
            print(f"  Trend: {paper.trend_title}")
            print(f"  Title: {paper.paper_title[:60]}...")
            print(f"  Year: {paper.year}")
            
    except Exception as e:
        print(f"\nâŒ TEST FAILED!")
        print(f"Error: {str(e)}")
        print(f"\nStack trace:")
        traceback.print_exc()
        
        
        
        # ë˜ëŠ” ì½”ë“œì—ì„œ
from bio_agent_code.agents.Trend_Profiling_Agent import run
result_state = run(state)

# ê¸°ìˆ  ìš”ì•½ í™•ì¸
for paper in result_state.academic_papers[:1]:  # ì²« ë²ˆì§¸ íŠ¸ë Œë“œ
    print(paper.tech_summary)