from __future__ import annotations
import os
import re
import traceback
from datetime import datetime
from typing import Dict, List, Optional

from dotenv import load_dotenv
load_dotenv()

# --- ì™¸ë¶€ ì˜ì¡´ ---
try:
    from tavily import TavilyClient
    _HAS_TAVILY = True
except Exception as e:
    _HAS_TAVILY = False
    print(f"[WARNING] Tavily import failed: {e}")

try:
    from anthropic import Anthropic
    _HAS_ANTHROPIC = True
except Exception as e:
    _HAS_ANTHROPIC = False
    print(f"[WARNING] Anthropic import failed: {e}")

# --- ë‚´ë¶€ ì˜ì¡´ ---
from ..state import PipelineState, TrendItem, PaperItem

# ---------------------------------------------------------
# ì„¤ì •
# ---------------------------------------------------------
PAPERS_PER_TREND = 10  # ê° íŠ¸ë Œë“œë‹¹ ìµœê·¼ ë…¼ë¬¸ 10ê°œ
SCHOLAR_SITE = "site:scholar.google.com"
RECENT_YEARS = 3  # ìµœê·¼ 3ë…„ ë…¼ë¬¸ ìš°ì„ 

# ---------------------------------------------------------
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ---------------------------------------------------------
def _norm(x: str) -> str:
    return (x or "").strip()

def _extract_year(text: str) -> Optional[int]:
    """í…ìŠ¤íŠ¸ì—ì„œ ì—°ë„ ì¶”ì¶œ (2020-2025 ë²”ìœ„)"""
    matches = re.findall(r'\b(202[0-5])\b', text)
    if matches:
        return int(matches[0])
    return None

def _parse_scholar_results(results: List[Dict], state: PipelineState) -> List[Dict]:
    """
    ì›¹ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë…¼ë¬¸ ì •ë³´ íŒŒì‹±
    
    ë°˜í™˜ í˜•ì‹:
    {
        'title': str,
        'authors': str,
        'year': int,
        'abstract': str,
        'citation': str,
        'url': str
    }
    """
    state.log(f"[Profiling] Parsing {len(results)} search results...")
    
    papers = []
    current_year = datetime.now().year
    cutoff_year = current_year - RECENT_YEARS
    
    for idx, r in enumerate(results, 1):
        try:
            title = _norm(r.get("title", ""))
            content = _norm(r.get("content", ""))
            url = _norm(r.get("url", ""))
            
            if not title or not url:
                state.log(f"[Profiling] Skipping result {idx}: missing title or URL")
                continue
            
            # ì—°ë„ ì¶”ì¶œ (ì œëª© ë˜ëŠ” ë‚´ìš©ì—ì„œ)
            year = _extract_year(f"{title} {content}")
            
            # ìµœê·¼ ë…¼ë¬¸ë§Œ í•„í„°ë§
            if year and year < cutoff_year:
                state.log(f"[Profiling] Skipping result {idx}: year {year} < cutoff {cutoff_year}")
                continue
            
            # ì €ì ì •ë³´ ì¶”ì¶œ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
            authors = ""
            if " - " in content:
                parts = content.split(" - ")
                if len(parts) > 1:
                    authors = parts[0][:200]  # ì €ì ë¶€ë¶„ ì œí•œ
            
            # AbstractëŠ” contentì˜ ì¼ë¶€ (ì²˜ìŒ 500ì)
            abstract = content[:500] if content else ""
            
            # ì¸ìš© êµ¬ë¬¸ ìƒì„± (APA ìŠ¤íƒ€ì¼ ê·¼ì‚¬)
            citation = _generate_citation(title, authors, year)
            
            papers.append({
                'title': title,
                'authors': authors,
                'year': year or current_year,
                'abstract': abstract,
                'citation': citation,
                'url': url
            })
            
            state.log(f"[Profiling] âœ“ Parsed paper {idx}: {title[:50]}... ({year or current_year})")
            
        except Exception as e:
            state.log(f"[Profiling] âœ— Error parsing result {idx}: {str(e)}")
            continue
    
    # ì—°ë„ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
    papers.sort(key=lambda x: x['year'], reverse=True)
    
    state.log(f"[Profiling] Successfully parsed {len(papers)} papers from {len(results)} results")
    
    return papers[:PAPERS_PER_TREND]

def _generate_citation(title: str, authors: str, year: Optional[int]) -> str:
    """ê°„ë‹¨í•œ ì¸ìš© êµ¬ë¬¸ ìƒì„±"""
    year_str = str(year) if year else "n.d."
    authors_short = authors[:100] + "..." if len(authors) > 100 else authors
    
    if authors_short:
        return f"{authors_short} ({year_str}). {title}"
    else:
        return f"({year_str}). {title}"

def _web_search_scholar(query: str, state: PipelineState, max_results: int = 20) -> List[Dict]:
    """
    Google Scholarì—ì„œ ë…¼ë¬¸ ê²€ìƒ‰ (Tavily API ì‚¬ìš©)
    """
    state.log(f"[Profiling] Starting web search for: '{query}'")
    
    if not _HAS_TAVILY:
        state.log("[Profiling] âœ— ERROR: Tavily library not available")
        return []
    
    api_key = os.getenv("TAVILY_API_KEY")
    if not api_key:
        state.log("[Profiling] âœ— ERROR: TAVILY_API_KEY not found in environment")
        return []
    
    try:
        client = TavilyClient(api_key=api_key)
        
        # Google Scholar ì‚¬ì´íŠ¸ ì œí•œ ê²€ìƒ‰
        search_query = f"{query} {SCHOLAR_SITE}"
        state.log(f"[Profiling] Search query: '{search_query}'")
        
        result = client.search(
            query=search_query,
            max_results=max_results,
            search_depth="basic",
            include_answer=False,
            include_raw_content=False
        )
        
        results = result.get("results", [])
        state.log(f"[Profiling] âœ“ Tavily returned {len(results)} results")
        
        formatted = []
        for r in results:
            formatted.append({
                "title": _norm(r.get("title", "")),
                "content": _norm(r.get("content", "")),
                "url": _norm(r.get("url", "")),
            })
        
        return formatted
        
    except Exception as e:
        state.log(f"[Profiling] âœ— ERROR during web search: {str(e)}")
        state.log(f"[Profiling] Stack trace: {traceback.format_exc()}")
        return []

def _fetch_paper_content(url: str, state: PipelineState) -> str:
    """
    ë…¼ë¬¸ URLì—ì„œ ì „ì²´ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
    """
    if not _HAS_TAVILY:
        return ""
    
    api_key = os.getenv("TAVILY_API_KEY")
    if not api_key:
        return ""
    
    try:
        client = TavilyClient(api_key=api_key)
        state.log(f"[Profiling] Fetching full content from: {url[:50]}...")
        
        # Tavily extractë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ë‚´ìš© ì¶”ì¶œ
        result = client.extract(urls=[url])
        
        if result and 'results' in result and len(result['results']) > 0:
            full_content = result['results'][0].get('raw_content', '')
            state.log(f"[Profiling] âœ“ Fetched {len(full_content)} chars from paper")
            return full_content[:5000]  # ìµœëŒ€ 5000ìë¡œ ì œí•œ
        
        return ""
        
    except Exception as e:
        state.log(f"[Profiling] âš ï¸ Could not fetch full content: {str(e)}")
        return ""

def _summarize_technology(trend_title: str, papers: List[Dict], state: PipelineState) -> str:
    """
    Claude APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë…¼ë¬¸ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ê¸°ìˆ  ìš”ì•½ ìƒì„±
    - ë…¼ë¬¸ ì „ì²´ ë‚´ìš©ì„ ê°€ì ¸ì™€ì„œ ë¶„ì„
    - ê¸°ìˆ  ì¤‘ì‹¬ì˜ êµ¬ì²´ì  ì„¤ëª…
    """
    state.log(f"[Profiling] Generating technology summary for: {trend_title}")
    
    if not _HAS_ANTHROPIC:
        state.log("[Profiling] WARNING: Anthropic library not available, using fallback")
        return _fallback_summary(trend_title, papers, state)
    
    api_key = os.getenv("ANTHROPIC_API_KEY")
    if not api_key:
        state.log("[Profiling] WARNING: ANTHROPIC_API_KEY not found, using fallback")
        return _fallback_summary(trend_title, papers, state)
    
    try:
        client = Anthropic(api_key=api_key)
        
        # ìƒìœ„ 3ê°œ ë…¼ë¬¸ì˜ ì „ì²´ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        papers_with_content = []
        for i, paper in enumerate(papers[:3], 1):
            state.log(f"[Profiling] Fetching content for paper {i}/3...")
            full_content = _fetch_paper_content(paper['url'], state)
            
            papers_with_content.append({
                'title': paper['title'],
                'authors': paper['authors'],
                'year': paper['year'],
                'abstract': paper['abstract'],
                'full_content': full_content if full_content else paper['abstract']
            })
        
        # ë…¼ë¬¸ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
        papers_context = "\n\n" + "="*80 + "\n\n"
        papers_context += "\n\n".join([
            f"[ë…¼ë¬¸ {i+1}]\nì œëª©: {p['title']}\nì €ì: {p['authors']}\nì—°ë„: {p['year']}\n\në‚´ìš©:\n{p['full_content']}"
            for i, p in enumerate(papers_with_content)
        ])
        
        state.log(f"[Profiling] Prepared {len(papers_with_content)} papers with full content")
        
        prompt = f"""ë‹¹ì‹ ì€ ìƒëª…ê³¼í•™ ë¶„ì•¼ ê¸°ìˆ  ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒì€ "{trend_title}" ê¸°ìˆ ê³¼ ê´€ë ¨ëœ ìµœê·¼ í•™ìˆ  ë…¼ë¬¸ë“¤ì…ë‹ˆë‹¤:

{papers_context}

ìœ„ ë…¼ë¬¸ë“¤ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ "{trend_title}" ê¸°ìˆ ì— ëŒ€í•´ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ê¸°ìˆ ì  ì„¤ëª…ì„ ì‘ì„±í•˜ì„¸ìš”:

## ê¸°ìˆ  ê°œìš”
(ì´ ê¸°ìˆ ì´ ë¬´ì—‡ì¸ì§€, ì–´ë–¤ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ”ì§€ 2-3ì¤„ë¡œ ê¸°ìˆ ì ìœ¼ë¡œ ì„¤ëª…)

## í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜
(ì´ ê¸°ìˆ ì´ ì‘ë™í•˜ëŠ” ì›ë¦¬ë‚˜ í•µì‹¬ ì ‘ê·¼ ë°©ì‹ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…. ì•Œê³ ë¦¬ì¦˜, ëª¨ë¸ êµ¬ì¡°, ë°ì´í„° ì²˜ë¦¬ ë°©ì‹ ë“± ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ í¬í•¨)

## ì£¼ìš” ì ìš© ë¶„ì•¼
(ì´ ê¸°ìˆ ì´ ì ìš©ë˜ëŠ” ìƒëª…ê³¼í•™ì˜ êµ¬ì²´ì  ë¶„ì•¼ë“¤ì„ ë‚˜ì—´)

## ê¸°ìˆ ì  íŠ¹ì§•
(ë…¼ë¬¸ì—ì„œ ì–¸ê¸‰ëœ ì´ ê¸°ìˆ ì˜ ê³ ìœ í•œ íŠ¹ì„±ì´ë‚˜ ì¥ì ì„ ê¸°ìˆ ì  ê´€ì ì—ì„œ ì„¤ëª…)

**ì‘ì„± ì›ì¹™:**
1. ë§ˆì¼€íŒ… ë¬¸êµ¬, ì¼ë°˜ë¡ ì  í‘œí˜„ ê¸ˆì§€
2. ë…¼ë¬¸ì—ì„œ ì–¸ê¸‰ëœ êµ¬ì²´ì  ê¸°ìˆ , ë°©ë²•ë¡ , ê²°ê³¼ë§Œ í¬í•¨
3. "í˜ì‹ ", "íšê¸°ì ", "ì§€ì†ì  ì—°êµ¬ ê°œë°œ" ê°™ì€ ì¶”ìƒì  í‘œí˜„ ì‚¬ìš© ê¸ˆì§€
4. ê¸°ìˆ  ìì²´ì˜ ì‘ë™ ë°©ì‹ê³¼ ê³¼í•™ì  ì›ë¦¬ì— ì§‘ì¤‘
5. ë…¼ë¬¸ ë‚´ìš©ì„ ê·¼ê±°ë¡œ ì‚¬ì‹¤ë§Œ ì„œìˆ """

        state.log("[Profiling] Calling Claude API for technical summary...")
        
        message = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=2000,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        
        summary = message.content[0].text
        state.log(f"[Profiling] âœ“ Claude API returned technical summary ({len(summary)} chars)")
        
        return summary
        
    except Exception as e:
        state.log(f"[Profiling] âœ— ERROR calling Claude API: {str(e)}")
        state.log(f"[Profiling] Stack trace: {traceback.format_exc()}")
        return _fallback_summary(trend_title, papers, state)

def _fallback_summary(trend_title: str, papers: List[Dict], state: PipelineState) -> str:
    """API ì‹¤íŒ¨ ì‹œ ë…¼ë¬¸ ì´ˆë¡ ê¸°ë°˜ ê¸°ìˆ  ìš”ì•½ ìƒì„±"""
    state.log("[Profiling] Using fallback summary generation (abstract-based)")
    
    if not papers:
        return f"{trend_title}: ë…¼ë¬¸ ë°ì´í„° ì—†ìŒ"
    
    # ë…¼ë¬¸ ì´ˆë¡ë“¤ì—ì„œ ê¸°ìˆ ì  í‚¤ì›Œë“œì™€ íŒ¨í„´ ì¶”ì¶œ
    abstracts = [p['abstract'] for p in papers[:5] if p['abstract']]
    
    summary = f"## {trend_title}\n\n"
    summary += f"**ë°ì´í„° ê¸°ë°˜:** {len(papers)}ê°œ ë…¼ë¬¸ ë¶„ì„ ({', '.join(str(p['year']) for p in papers[:5])})\n\n"
    
    # ìƒìœ„ ë…¼ë¬¸ ì œëª©ê³¼ ì´ˆë¡ ìš”ì•½
    summary += "**ì£¼ìš” ì—°êµ¬ ë‚´ìš©:**\n"
    for i, paper in enumerate(papers[:3], 1):
        summary += f"{i}. {paper['title']}\n"
        if paper['abstract']:
            # ì´ˆë¡ì˜ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ (ì²« 150ì)
            core_abstract = paper['abstract'][:150].strip()
            summary += f"   - {core_abstract}...\n"
    
    summary += f"\n**ê¸°ìˆ  ë¶„ë¥˜:** {trend_title} ë¶„ì•¼\n"
    summary += f"**í™œì„± ì—°êµ¬ ê¸°ê°„:** ìµœê·¼ {RECENT_YEARS}ë…„\n"
    
    return summary

def _print_results_summary(state: PipelineState) -> None:
    """
    í„°ë¯¸ë„ì— ìµœì¢… ê²°ê³¼ ì¶œë ¥
    """
    print("\n" + "="*80)
    print("ğŸ“Š TREND PROFILING AGENT - RESULTS SUMMARY")
    print("="*80)
    
    if not state.academic_papers:
        print("âš ï¸  No academic papers collected")
        return
    
    # íŠ¸ë Œë“œë³„ë¡œ ê·¸ë£¹í™”
    trends_data = {}
    for paper in state.academic_papers:
        if paper.trend_title not in trends_data:
            trends_data[paper.trend_title] = []
        trends_data[paper.trend_title].append(paper)
    
    print(f"\nâœ“ Total Trends Analyzed: {len(trends_data)}")
    print(f"âœ“ Total Papers Collected: {len(state.academic_papers)}")
    print("\n" + "-"*80)
    
    # ê° íŠ¸ë Œë“œë³„ ìƒì„¸ ì •ë³´
    for idx, (trend_title, papers) in enumerate(trends_data.items(), 1):
        print(f"\n[{idx}] TREND: {trend_title}")
        print(f"    Papers Found: {len(papers)}")
        
        # ì—°ë„ ë¶„í¬
        years = [p.year for p in papers]
        year_counts = {}
        for y in years:
            year_counts[y] = year_counts.get(y, 0) + 1
        print(f"    Year Distribution: {dict(sorted(year_counts.items(), reverse=True))}")
        
        # ê¸°ìˆ  ìš”ì•½ ë¯¸ë¦¬ë³´ê¸°
        if papers and papers[0].tech_summary:
            summary_preview = papers[0].tech_summary[:150].replace('\n', ' ')
            print(f"    Tech Summary: {summary_preview}...")
        
        # ìƒìœ„ 3ê°œ ë…¼ë¬¸
        print(f"    Top Papers:")
        for i, paper in enumerate(papers[:3], 1):
            print(f"      {i}. {paper.paper_title[:60]}... ({paper.year})")
            print(f"         Authors: {paper.authors[:50] if paper.authors else 'N/A'}...")
            print(f"         URL: {paper.url}")
    
    print("\n" + "="*80)
    print("âœ“ Profiling Complete - Data saved to state.academic_papers")
    print("="*80 + "\n")

# ---------------------------------------------------------
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ---------------------------------------------------------
def run(state: PipelineState) -> PipelineState:
    """
    Trend Profiling Agent
    
    í”„ë¡œì„¸ìŠ¤:
    1. state.discoveredì—ì„œ íŠ¸ë Œë“œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    2. ê° íŠ¸ë Œë“œì— ëŒ€í•´ Google Scholar ê²€ìƒ‰
    3. ìµœê·¼ ë…¼ë¬¸ 10ê°œ ì¶”ì¶œ (ì œëª©, abstract, ì¸ìš©êµ¬ë¬¸)
    4. ë…¼ë¬¸ ê¸°ë°˜ ê¸°ìˆ  ìš”ì•½ ìƒì„±
    5. state.academic_papersì— ê²°ê³¼ ì €ì¥
    6. í„°ë¯¸ë„ì— ê²°ê³¼ ì¶œë ¥
    """
    print("\n" + "="*80)
    print("ğŸ”¬ TREND PROFILING AGENT - START")
    print("="*80)
    
    state.log("[Profiling] ========== START ==========")
    state.log(f"[Profiling] Timestamp: {datetime.now().isoformat()}")
    
    if not state.discovered:
        state.log("[Profiling] âœ— ERROR: No trends to analyze (state.discovered is empty)")
        print("âš ï¸  No trends found in state.discovered - Skipping profiling")
        return state
    
    state.log(f"[Profiling] Found {len(state.discovered)} trends to analyze")
    print(f"âœ“ Processing {len(state.discovered)} trends from Discovery Agent\n")
    
    # API í‚¤ ì²´í¬
    tavily_key = os.getenv("TAVILY_API_KEY")
    anthropic_key = os.getenv("ANTHROPIC_API_KEY")
    
    state.log(f"[Profiling] TAVILY_API_KEY: {'âœ“ Found' if tavily_key else 'âœ— Missing'}")
    state.log(f"[Profiling] ANTHROPIC_API_KEY: {'âœ“ Found' if anthropic_key else 'âœ— Missing'}")
    
    if not tavily_key:
        print("âš ï¸  WARNING: TAVILY_API_KEY not found - search will fail")
    if not anthropic_key:
        print("âš ï¸  WARNING: ANTHROPIC_API_KEY not found - using fallback summaries")
    
    all_papers: List[PaperItem] = []
    
    # ê° íŠ¸ë Œë“œ ì²˜ë¦¬
    for trend_idx, trend in enumerate(state.discovered, 1):
        trend_title = trend.title
        
        print(f"\n[{trend_idx}/{len(state.discovered)}] Processing: {trend_title}")
        state.log(f"[Profiling] ========== TREND {trend_idx}/{len(state.discovered)}: {trend_title} ==========")
        
        try:
            # Google Scholar ê²€ìƒ‰ ìˆ˜í–‰
            state.log(f"[Profiling] Step 1: Web search")
            results = _web_search_scholar(trend_title, state, max_results=20)
            
            if not results:
                state.log(f"[Profiling] âœ— No search results for: {trend_title}")
                print(f"  âš ï¸  No search results found - skipping")
                continue
            
            # ë…¼ë¬¸ íŒŒì‹±
            state.log(f"[Profiling] Step 2: Parse papers")
            papers = _parse_scholar_results(results, state)
            
            if not papers:
                state.log(f"[Profiling] âœ— No valid papers after parsing for: {trend_title}")
                print(f"  âš ï¸  No valid papers found after parsing - skipping")
                continue
            
            print(f"  âœ“ Found {len(papers)} papers")
            
            # ê¸°ìˆ  ìš”ì•½ ìƒì„±
            state.log(f"[Profiling] Step 3: Generate tech summary")
            tech_summary = _summarize_technology(trend_title, papers, state)
            
            # PaperItem ìƒì„±
            state.log(f"[Profiling] Step 4: Create PaperItem objects")
            for paper_idx, paper in enumerate(papers, 1):
                paper_item = PaperItem(
                    trend_title=trend_title,
                    paper_title=paper['title'],
                    authors=paper['authors'],
                    year=paper['year'],
                    abstract=paper['abstract'],
                    citation=paper['citation'],
                    url=paper['url'],
                    tech_summary=tech_summary
                )
                all_papers.append(paper_item)
                state.log(f"[Profiling] Created PaperItem {paper_idx}/{len(papers)}")
            
            print(f"  âœ“ Generated tech summary ({len(tech_summary)} chars)")
            state.log(f"[Profiling] âœ“ Successfully processed trend: {trend_title}")
            
        except Exception as e:
            state.log(f"[Profiling] âœ— CRITICAL ERROR processing {trend_title}: {str(e)}")
            state.log(f"[Profiling] Stack trace:\n{traceback.format_exc()}")
            print(f"  âœ— ERROR: {str(e)}")
            continue
    
    # ìƒíƒœì— ì €ì¥
    state.academic_papers = all_papers
    state.log(f"[Profiling] ========== COMPLETE ==========")
    state.log(f"[Profiling] Total papers collected: {len(all_papers)}")
    state.log(f"[Profiling] Trends processed: {len(set(p.trend_title for p in all_papers))}")
    
    # í„°ë¯¸ë„ì— ê²°ê³¼ ì¶œë ¥
    _print_results_summary(state)
    
    return state
